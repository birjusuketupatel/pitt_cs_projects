hello hello I'm doing great
hold on are you gonna make your
microphone stuff louder is that better
yeah oh hey yeah huh okay you have the
email or whatever uh yeah I do
oh sorry this is the first time I've
done something like this so I apologize
I'm a little nervous yeah no problem
it's fine all right real quick just to
establish what is your uh what is your
background
sure so I'm a current senior in
philosophy at UC Berkeley I am
graduating in a month and a half give or
take I am on track to graduate summa [ __ ]
laude so what does that mean
it's nice top of your class or whatever
okay yeah so top 4% of my class um what
is and then what is like your specialty
do you have that as an undergrad in
philosophy or is it just a broad general
yeah so broadly speaking you study
everything um it's really just sort of
personal interests so my personal
interest is definitely ethics and
political philosophy I'm really
interested in questions that sort of
Rawls covers and stuff like that right
yeah it's a little bit more meta ethical
than I like to get mostly because I find
a lot of these discussions to be kind of
misguided at the end of the day but yeah
I agree that yeah it's it's definitely
up my alley
which is why listening to these people
talk about the graph made me a little
bit skeptical okay all right cool um and
then I have no formal background in this
whatsoever I just talked to people like
you to get my opinion so don't you sure
okay okay best all right so let me just
read this okay yeah I wanted to add my
take is a semi-active professional
philosopher to the conversation you had
last night with a why full discloser I
talked with the two of them frequently
and they initially invited me to come on
and discuss last night with you I was
traveling for work I don't defend their
views of ethics though I do think one of
your criticisms is misguided I'm going
to it our most deeper conversation and
focus on one particular issue the issue
in contention is whether the poly
axiomatic ethical system is just use a
little Terry
so this is kind of my I know that she
puts this in quotes but this is just
kind of my first thing so I tried to
search for this has does anybody use the
term poly axiomatic is this like even a
real thing because I can't find like
anything yeah no I've never I've never
heard of anything put like that the
closest thing that I'm aware of that
gets to the idea is value pluralism yeah
but that was different from monism yeah
yeah and I but I don't think we need
when you say hold on one second I got a
[ __ ] fantasy when you say value
pluralism that's the values that you're
talking about there that doesn't have to
do with utilitarianism and deontology
does it well it has to do with the
fundamental values that both of those
principles so utilitarianism and
deontology refer to ethical frameworks
yeah but I don't actually have values in
and of themselves right well they're
built on values right so utilitarianism
is built on values like consequences
pertaining to the pleasure and pain
experienced by rational agents they
ontology is built on values having to do
with the respect that is owed to
rational agents outside of consequences
so they do have to do with values but
they go beyond the values let me try to
be like really clear when I say they
don't have they don't necessarily have
values they themselves don't imply value
so for instance like if I were to say I
am a dant ologist that doesn't
necessarily mean that I think murder is
wrong what it means is that if I think
murder is wrong I can interpret it
through a danto logical framework or
utilitarian framework right like just
saying oh yeah okay okay that's what I
mean okay yeah they don't prescribe
particular action just through the word
itself they prescribed formulas that get
us to answers a gotcha gotcha okay cool
cool so you would have a set of values
first and then you would interpret that
through the framework of de ontology or
consequential or daljeet consequence for
them right I mean that would probably be
the wrong way to go I think both of the
I think most philosophers would say you
should have the framework first and then
use that framework to arrive at your
value system it's sort of motivated
reasoning if you go the other way
okay sure okay okay yeah cool I can
understand it my second part then would
be neither of these things or axioms are
they or are they utilitarianism or
deontology
neither of them are
axioms per se they're built on axioms so
I guess you could boil them down to
really sort of brewed axioms like
pleasure - pain is the right thing to do
or you should always take the action
that is considered to respect agents to
the maximal degree so you can boil down
the systems to certain axioms but you
would probably be doing huge injustice
to the framework in doing so sure well
and then technically you can boil
everything down to an axiom by
definition right yeah yeah which is why
it's problematic okay guys yeah which is
why also the graph doesn't seem to make
sense for me like utilitarianism
probably wouldn't look this pretty
depending on what kind of utilitarianism
you're talking about you're not just
talking about well-being - suffering you
could be talking about total utility
average utility maximal average utility
there are so many different ways to
construe it that by only having one
variable on the graph you open up a lot
of questions
sure okay cool are you familiar with the
term threshold Anthology or I'm not
entirely sure what specifically that's
referring to it seems like it's a
modification on classical Conte anism
where n't you say that you know an
action is right or wrong
unless you're having to choose between a
conflicting action that also has to do
with the respect that is owed to a
particular agent gotcha so this is
something that I think I'm familiar with
okay so if I cut you off I'm just trying
to talk to make sure I have the right
idea before you get in a rut yeah so I'm
not trying to cut you ought to be
reduced tell me tell me if I don't
really have yeah I don't really have
thoughts on threshold de ontology and
I'm not entirely well-versed on it so I
just wanted to sort of put that out
there gotcha okay that's fine so my
understanding of this concept I think I
familiar with is like a classical
challenge to a day intelligence might be
something like Nazi comes to your door
knocks on the door and says are there
any Jews hiding your basement well the
Dan tile just has an imperative not to
lie because honesty is virtuous or
whatever so the so the day intelligence
has to tell the truth which would result
in a Jewish person being killed so I've
heard there are other forms of kind of
augmenting deontology so that you can
respect other agents in a way where
right telling the truth doesn't mean
killing someone right yes so that that
kind of that sounds like what this is
kind of getting at
when you talk about threshold dant ology
or whatever is that if you're gonna
write yeah it might be more helpful to
think of instead of it being a threshold
rather you're just weighing conflicting
obligations that you have so you have an
obligation not to lie to the Nazi but
you also have an obligation to the
people that are hiding in your house as
they are like every party involved is a
rational agent so then you're trying to
weigh what obligations you might have to
them and that's why I think it's more
helpful to understand these kind of
questions through a contractual astray
Merck rather than a strictly classical
Continent framework it is clearly okay
so then continuing on the email it is
clearly not utilitarianism is
simpliciter does that I guess it's a
fancy way of saying simply utilitarian
classical note that anything about
utility about utilitarian we should
general it's consequentialism
I take the following send its main
accurate description of HA okay um so
can I just skip to the end here I think
broadly speaking uh-huh the email is
correct but it doesn't matter so so this
is something that I don't understand so
this is what I have to understand what I
don't get is what so I'm gonna ignore
the fact that it really bothers me that
like phrases are being used here that I
can't find anywhere on the Internet
it's like poly axiomatic ethics this is
a phrase that I can't find anywhere in
anything but no one is any writing on
this whatsoever
well the purposes of the discussion you
can just understand that as referring to
the graph okay sure yeah I guess just
bothers me that if two people with no
background have created like a new
framework for philosophy it rubs me the
wrong way in a lot of different ways
right it's not a framework they're
trying to take some weird spin off of
value pluralism but without engaging
with any of the problems of pluralism
gotcha
or this or maybe threshold ant ology as
well maybe uh so I'm not entirely sure
so it looks like threshold de ontology
is being used here as a synonym but I'm
not sure how it's a synonym because they
ontology the reason why you it's
permissible under some interpretations
of the anthology to lie to the Nazi at
the door
does not have to do per se with
consequences it has to do with
obligations so I'm not even sure how you
would put de ontology as a threshold
concept into the term
they're trying to use in the graph okay
so my problem then with with the thing
that these people with this email is so
if I skip down to like um this statement
is pretty clearly into utilitarian
because it allows for the following true
ethical statements X is obligatory even
though it occurs negative fifty units of
utility X is not obligatory even though
it could it creates five hundred units
of utility how can you take add ant
illogical obligation and measure it in
terms of utility without it being you
thought you can't have so yeah that's
the biggest problem that I had with the
graph so here's I'm trying to find a
good way to put this so they tried to
get around this problem by saying uh
well just replace the ontology with
whatever D could be like the inherent
moral worth of an action but that phrase
moral worth of an action becomes
meaningless insofar as you throw a
ontology out the window so insofar as a
ontology or at least most forms of a
ontology would say that the obligation
that you have to perform or not perform
a specific action is binary not a
gradient so you either can or can't
perform it it's not case-by-case can you
perform it then you can't really
quantify whether something is more de
ontological or less down to logical the
way that you could try to make that work
is by saying instead of a ontology you
make a contractual ism do you know what
I mean when I say contractual ism no
okay so contractual ism you can be
thought of as almost a spinoff off of
the traditional content view basically
the idea is we things to other people
and that's where morality is grounded in
so if I make a promise to somebody then
that obligation to keep the promise
might outweigh my desire to just stay
home so it's not utilitarian right it
doesn't matter what brings you the most
pleasure ever met it matters what the
complications in our obligations are
guys sure yeah
now importantly and this is why it sort
of ties into day ontology even if you
haven't explicitly made a promise to
people you have obligations to other
people in virtue of them being human and
part of like your social group right so
I might not have explicitly promised not
to kill you
but I'm not justified in killing you I
have that obligation sort of de facto
right
kind of approaching like a categorical
norm or something or imperative sort of
the the main deviation between
contractual ISM and a ontology is that
you can weigh those obligations to
decide what actions you altima take so
the main reason why this is considered
better than des ontology in my view is
that it allows you to weigh obligations
that you have to people that you care
about more than strangers so it's better
to keep a promise to my mother than it
is to keep a promise to a random
stranger because of the obligations I
have to my mother right gotcha could
aunt Ella just argue that you have
stronger obligations or whatever to
people that you know versus strangers or
no-cut doesn't really have the tools to
do that in my view that insofar as
they're all reasonable agents in this
set setting then it doesn't matter right
because you have obligations equally
people this is this is the fundamental
problem with mapping de ontology to
utilitarianism like the graph does is
because de ontology fundamentally
assumes that you cannot interchange
people you can't say I'm going to help X
person at the expense of y person
because hey they're interchangeable
right pleasure is pleasure pain is pain
that's not okay
contractual ism says there are
meaningful differences between people
that allows that that changes the nature
of our obligations to them so the reason
why I bring up contractual ism is
because that's one way that you could
potentially order the obligations that
you have to other people such that you
can maybe make some kind of gradients I
don't know how you would put numbers on
that but it certainly would make a lot
more sense than de ontology which just
says you have an obligation period and
it's not a more-or-less obligation
gotcha and then I okay and then just so
I made the statement but I don't
actually know i shouldn'ta made this
statement in Indian Tala G is one action
more wrong than another or do you just
have binary rights and wrongs like say
like murdering one person person versus
murdering two people yeah my
understanding is that and again I could
be wrong I'm not a Content philosopher
but my understanding so the reason why
the dant ologist says in the trolley
problem
don't touch the lever is because you
cannot engage in a system that's going
to be using another as an means to an
end
so even though you could save one person
as opposed to saving five and so you
could argue through your inaction you're
killing the five the deontologist says
no you're refusing to engage with that
thing to do because it is wrong to pull
the lever in either direction
okay wait and then I think I missed the
very beginning of that so would you say
then that they're just binary rights and
wrongs wasn't it more wrong or more
right than another wrong or right my
understanding would say yes yes okay
gotcha
just entangling
I guess I'm kind of confused cuz you
said you broadly agree the email but it
seems like so it seems like we both
agree that we disagree that dant
illogical obligations can be measured in
units of utility right we agree that
that's probably not true
correct I the reason why I agree broadly
speaking is because I'm charitably under
understanding the email says if insofar
as you have some framework that gives
actions some kind of inherent moral
worth then that will that framework will
lead you to a different conclusion yeah
yeah this is my question okay is that
yeah if you have quote unquote inherent
moral worth but then you ascribe that
inherent moral worth some utility so
that you can measure it against other
utility I don't know I guess I'm just
not understand how that's not just
utilitarianism does it does it what'd I
just say makes sense I think so so the
the main thing that confused me about
the email is it seems to me that there's
a confusion in terms because what what
they're trying to do is say that there
is a different framework that so okay
actually maybe I can just answer your
question more directly so utilitarianism
cares only about pleasure and pain right
that's that's the primary thing that's
driving us here using utility in a
seemingly different term
yeah actually well actually let's let's
do this real quick because I might be
wrong here then so I'm not aware that I
thought I thought that utilitarianism
measure
utility where utility can be more
broadly defined than just pleasure and
pain is that not true uh yes it can so
that that gets the distinction between
like the very very traditional
utilitarian coming out of like Bentham
and the more modern form of
utilitarianism that comes out of mill
mill says it's not about pleasure and
pain so much as it is about happiness
yes and where that even happiness
Flexcon yeah it can be very broadly so
like maybe you're happy not maximizing
like maybe a soldier jumping in front of
a grenade might find happiness in that
or whatever even if it doesn't maximize
or something like that right that
happiness doesn't necessarily map onto
strict pleasure and pain or something
well sort of so Mill defines happiness
is sort of define being defined by
higher and lower pleasures lower
pleasures being more in the vein of like
the Bentham pleasures food sex whatever
higher pleasures being more like
intellectual stimulation being able to
sort of find fulfillment in other areas
of your life so it's just a more complex
form of pleasure I suppose you could say
sure how about like it's a better
example somebody becoming a priest and
forgoing sexual activity like even
though I was having lots of sex I'd be
seen as pleasurable yeah the higher
fulfillment or whatever okay yeah Mills
totally on board with discounting lower
pleasures in exchange for higher
pleasures here it seems like they're not
using utility in either of those senses
they mean utility and this is the
problem with the graph and with moral
pluralism more broadly right the the
bigger problem here is that they assume
that there is at some point a place
where the value of that what they're
what they're defining as they ontology
is exceeded by the value of
consequentialism yeah that bright-line
appeals to a third framework that is yet
undefined that says at this point it is
now worth it to take the action or not
take the action okay so insofar as they
do that the utility that the email is
referencing seems to be talking about
utility on that third framework that is
able to met say look that utility isn't
the same as utilitarian utility insofar
as it's going to be
taking you in a different direction so
maybe it's easier to talk in terms of
variables like if we're calling the
variable that the graph is going to spit
out as like the value of an action alpha
and the utility that a strictly Tillet
Arianism would ascribe to the action
beta beta is not going to be the same as
alpha right because they're using
something else to modify alpha another
framework um but it seems like an order
to get to UM in order to arrive at alpha
