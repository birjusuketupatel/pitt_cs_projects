it's well it's a winner-take-all
scenario whoever invents the general AI
is going to win the world basically and
that goodbye soon yeah we're like really
close to it aren't we uh yeah I mean if
you're well they theorized 30 to 50
years
but and that's that's incredibly soon
even if it's that which is in some cases
a conservative estimate there were still
hardware limited though that's the big
problem
like the whole idea of it spiraling out
of control with our current hardware
isn't feasible well how's the hardware
limited how was it Hardware limited
because of the cost efficient well
partially what do you want you can
explain it better
oh I was gonna say partially because of
yeah cost and everything but no if you
talk about world wide spread then you're
no longer talking about cost do you know
I can get anyway hold on no you said it
was Hardware limit how is it hardly
limited you realize how much processing
an AI has to do before it can actually
become a true soft day i right
I mean I I'm kind of like a true soft AI
and I don't need like millions and
millions of energies of processing no
well you have millions and millions that
you I don't know the exact number but
you have neurons exactly but I mean I
say to you you're telling me that the
biggest I have more processing power on
my brain than the most complicated
powerful see like thing in the world I
think I've heard it's a software problem
not a hardware one but then why did you
just agree earlier with him when he said
it was hardware limited and now you're
telling me I spiraled out control is
Hardware limited with our technology now
yes you could do that but it wouldn't
spiral out of control in an instant such
as people theorize he was expecting
earlier about quantum computers yeah I
want them computing is that there that
one is basically but I yeah well
basically he was exciting how on
computers they fire on what is it called
again like it's basically an infinite
number of the connections but there you
could argue their equivalent of threads
yeah but it's only forty percent
accuracy because of the fact that
they're don't connects like a normal
computer
well quantum computing to I'm not sure
the exact number
but I know for a while there the current
accuracy of an actual bit received too
bit transmitted was only about 30-40
percent right but but anyways listen
like that you should you should I don't
know if you've heard I mean look the
fact that Stephen Hawking the fact that
Bill Gates Elon Musk I mean there's so
many people who are experts and just in
in the field itself and I'm not an
expert and AI know Bill Gates no but hey
I know okay I can cite there there are
experts in the field as well but they're
just lesser-known people like stupid me
these are people who work in the field
and are talking extensively about how
this is an imminent threat I mean I they
know tomorrow it's gonna be a threat but
in the coming years your future you know
30 to 50 years maybe well here's the
thing
consider this this is a problem that has
incredibly far-reaching implications if
so in other words a general AI has
incredible power incredible implications
of which we've never seen before it's a
winner-take-all scenario to develop it
and so this is a problem that we don't
know how far off it is
but more importantly we don't know how
long it would take us to tackle that
problem for all we know this is a
problem that could take ten years of
people really the right people really
focusing on it and all we have is 15 or
20 years that's an example so it's
something where we're really on the
clock about it so you think that one day
somebody's just going to flip the switch
and we're magically going to come up
with some AI that's going to be ready to
take over the way it's not gonna be like
a progression take thirty to fifty years
is because the fact by the time it takes
that many iterations for the air I
execute like that's that's the hardware
limit if we had a computer that could do
infinim out parallel processes today
like who knows that could be five years
from now that I would know what is
infinite amount of parallel processes
when are we ever getting these computers
that's the argument that's the hardware
limit if we had something that allowed
us to do so many parallel processes we
could cut the the time span between us
having a heart AI today to us having a
soft day tomorrow I'm sorry crisis
I thought the sewer first of all I'm
pretty sure quantum computers only give
you like an order of magnitude more
processing power it's not like yeah and
today this quantum computers are trashed
so secondly I thought that typically
when we were speaking about a eyes these
are problems with algorithms these
aren't problems with like RAW processing
power like I'm pretty sure we can make a
computer that is like it has the
infinitely more processing power than
any individual human brain I'm pretty
sure we have them right now in our homes
but it's yeah I think it's about we
develop a say actually they say that IBM
Watson's its current rate will become a
soft AI in the 30 50 year times I think
I'm pretty skeptical of anything that's
trying to predict tech out even past 15
yeah oh about AI to actually be able to
give a definite answer okay sounds like
a problem that I shouldn't be extremely
concerned about right now that you
should know no no no no no no doesn't
that just give credence to the idea that
in 10 to 15 years we could have some
type of technological breakthrough that
could get us to a soft AI hello I could
be wrong but I don't think correct me if
I'm wrong because you're the expert but
I've never heard of any massive
breakthrough in AI it seems like it's a
lot of grueling work on many iterations
over and over again of slowly improving
different learning techniques in order
to work generally is better so I don't
think there's ever gonna be a time where
we go from having a computer that's
really good at chess - oh my god it's
trying to fire all the nooks and take
over the world is it self identified
than its best interest rate limit it
seems like a huge yeah so the way you're
saying that would it be described as the
old space race with a Cold War because
now we're just doing it for the AI know
and if anyone reaches that AI do they
have the potential to do that a war game
scenario what I don't even know what
that one war game scenario which is what
you just said which is launching all the
nukes towards each other and all that
[ __ ] yeah that's what it makes it sound
like you guys that's where okay guys
kind of like magically real quick yeah
honestly it's just fear-mongering
because this is where I was saying the
AI is not going
to spiral out of control in the fashion
the media wants you to believe way but
that's the amount of patient even do a
calculation for AI is absurd like you're
not gonna have AI hop onto your
smartphone and suddenly be able to
control your vehicle or anything like
that in the fashion that that means it
makes it out to be no one's making that
argument but what yeah the point I'm
saying is that to get that when you get
to it generally I all bets are off the
table I think that's pretty
well-established like we got we already
have like general learning AI don't know
what you a generally AI is in a self
improving AI that is that's how we know
isn't that how we taught the current a
eyes the google deepmind or whatever
isn't that isn't that why you give it a
you give it a task and then you don't
tell it how to complete it and then it
iterates like a process to get there in
the most efficient way possible isn't it
- definitely we can't rewrite our source
code what do you I don't notice how you
mean I didn't say we could
what is it wasn't rewriting your source
code have to do with anything we don't
have a source code I mean we're not yeah
but why would an AI need to be able to
rewrite its source code was I mean DNA
could be your source code limiting its
capabilities and what I'm saying is the
general AI is something where it is
proficient in a multitude of different
fields not just one specialized task
like Watson is for example can I do
controlling your [ __ ] missile defense
systems yeah sure I mean it can do a
variety of different tasks efficiently
better than any human can that's that's
when people are referring to generally I
that's what they generally are referring
to from what I've heard from the experts
in the field that I'm listening to but
anyways wait to answer your original
question you were you were saying you
you said well it seems you said well I
guess that would depend how you define
that but I mean what the things we're
doing now and we can do now with AI has
been is crazy leaps from you know what
was it sure I'm sorry when I hear like
the big alarmist talks like when people
like you talk about this you make it
seem like there's gonna be like some day
we're gonna go from an AI that can do
this to like a massive breakthrough that
that but it seems to me that that's not
how
in this field has worked it's actually
just been a lot of grueling work very
small incremental improvement jump over
jump of things getting slightly better
as time goes on so there's never night
there's never gonna be like the massive
AI break that we're all of a sudden like
something comes out of nowhere we flip a
switch and there's a huge AI thing but
though if we let this problem if we let
this thing go unchecked and if we let it
become an arms race of people racing to
be the first one to create this truly
super intelligent general AI which is
very well could because there's so much
incentive to do so
then that problem could could definitely
arise where obviously look you're right
it's not gonna happen overnight it's not
like it just oh this guy figured how to
do it you broke the code but the problem
is when you have a lot of groups working
towards this and we don't you know have
a way to pump the brakes and be like
whoa let's before we [ __ ] design this
next super crazy intelligent thing we
should figure out the implications of
what this really like they don't have
the protocols set in place to see yeah
there's no protocol exactly there let me
get a little bit closer to it actually
being a thing maybe we will but like
when you're talking about something that
is theorized to be 30 to 50 years out
which one when somebody says something
is 30 to 50 years a ton Tech what it
tells me is they have absolutely no idea
when it's gonna be there nobody knows
what anything attack is gonna look could
also be super well if it was sooner they
would say sooner not 30 to 15 years out
there like that's probably why nobody's
like taking it very sick like for
instance like there's going to need to
be like very big ethical discussions and
repercussions and [ __ ] for genetically
modifying super soldiers and [ __ ] but
we're probably really [ __ ] far out
from that [ __ ] as well so we're not
having those conversations either why
because well that's because this
particular conversation is something
that pretty much trumps everything else
I'm really what if you could genetically
modify and create like super thinking
suit like everybody's becomes like
Captain America that society you'd
probably be I mean that's okay that's
that's an important topic obviously but
what I'm saying is a super intelligent
generally AI is something that trumps
everything else in terms of power and
importance you've got a bunch of Captain
America's running around that can run
covert ops missions and like blow up
these abilities of what if well what
about the super American or what about
the super Captain America soldiers that
confuse Bionic chips into their brains
they can say they can merge with the
general AI
you're right you bigger good [ __ ] to say
but what I'm saying is like imagine
different countries working on their own
version of Captain America okay no no I
understand what you're saying it's a
race what I'm saying is what if somebody
is while all these countries are wasting
time working at AI somebody's working on
a human that can plug the AI chip in and
they're gonna be the ones that win
you're referring to basically versus
that's the element Elon Musk neural
sleeve or whatever idea which is you
know that our base our best solution is
to merge with the AI essentially the
problem with that is you would think the
problem with that is in order to develop
that neural sleeve I figure what he
calls it but in order to develop that
you would first you know assuming you
would first need to create the AI in
order to put it onto your brain yeah but
when these guys will do is they'll wait
for other people to make the eye and
then they'll make of the super people
first but it'll be USB type n it plugs
into your exactly so they'll be
requesting the api's and [ __ ] from them
so that they can make their USB type end
specs so that as soon as their AI is
released they'll be able to plug it
right in and they'll be able to combine
the AI with theirs Captain America jeans
and it'll be Oliver you seem to think
that this is something that just good
it's gonna progress linearly we're all
gonna have an eye on it we're gonna be
able to keep it safe which is not
necessarily the case what I think is
that this issue is so far out that like
talking about is is absurdly
hypothetical that that is so if it's if
that was the case then why are people
who are look you might say well they're
not technically ok there are experts in
the field who are saying these are
things that are not that far off that
are really important that okay very good
I don't want to do this I don't want to
be like a jerk but can you link me like
a paper where somebody's right I just
want to read in their words where
they're saying that like this is gonna
happen soon and not and so when I read
this paper it's not gonna be like hey
guys these are some things that you
might want to think about because
potentially very far down the road these
are going to be issues that we might run
into right it's not gonna be like that
it's gonna be like AI general AI is
gonna be a big thing really soon five
years from now
yeah Stuart Russell is a pretty
prominent figure in the film
I'll find a an article thanks Stuart
Russell tennis Oh artificial
intelligence okay yeah
hey yo Funkmaster Nick Bostrom is
another guy he works with he's not a
like a technician or something but he
works with and talks with a lot he's a
very uh I guess I put I post it into the
politics the shittiest way to compare
freedom I agree into AI that's and I
don't think completely discrediting
people like Elon Musk or Steve Wozniak
and Stephen Hawking also say that I was
like [ __ ] Elon Musk Wozniak Billy
these guys have no background in AI
develop why would I yeah I know but
airway therapy doesn't think it was an
argument of appeal to Authority for a
lot of people they see someone like
Stephen Hawking saying AI is bad we
should stop it
so they're like oh he's an expert on one
field Iman I don't think it's an appeal
to Authority to just say well no if
you're if you're basing your entire this
is the definition of art of fallacious
appeal to Authority well no I'm not
saying I'm saying that's like your
entire life you know just even saying
like oh I think he has that's nice
fallacious is it it's just adding to the
fact that this is I'm not the only one
thinking about this obviously I didn't
get that I didn't formulate these ideas
or this this argument it's just right
you know other people are concerning
Hawking isn't the AI but if you what if
he was a person that made the I never
know man what if that's the reason he
never stands up and then and then people
I don't know if you're you would give
this argument destiny but people would
often say things like well you can just
unplug it you can just you can just have
the kill switch and it's all good but
like that is so that is just that's
not can you just give me the scenario
like you're making it sound like the AI
is gonna magically take over why
wouldn't it be done in like in like a
disconnected environment like a land
environment like a single server like
why do you think that all of a sudden
were doesn't magically alright and they
give it control of all the missiles and
then it's just gonna flip a switch a
takeover let me give you a perfect
example okay so let's say let's say we
do have this AI that's designed like you
said in like a LAN area right back to
the internet isn't it it's a closed box
okay so the problem there is assuming
this AI is super intelligent is greatly
out vastly intelligent than anyone else
that means it can think it can do things
in an incredible scenarios right so for
example this is just me thinking with my
simple human brain compared to it but
for example it could fake having a
[ __ ] malfunctioning virus or
something it needs maintenance during
that time of people opening it up doing
some sort of maintenance dick it in some
way [ __ ] you know alter its source
code it can do something that will have
an unintended consequence of us doing
something oh we're just fixing it it
could stage stuff like that you could do
social engineering like up the [ __ ]
ass like dude this thing is literally
gonna be a god in terms of Intel okay so
even if I give you let even if I get
grant you all of that which is a huge
stretch all you have to have our oh god
what do they call it in um in it when
you've got like a clearance when you're
when you're in like not clean rooms but
there's like there's a name what air gap
yeah you have like air gaps places where
you're not allowed to bring in or out
USB port are USB drives or anything like
that you're not allowed to have internet
or Wi-Fi or anything like that yeah like
air gap places like like how is it mind
went crazy that's that's what people
fear they don't fear of I don't think
the white gap scenarios oh there's one
more snare there's no there's there's
endless scenarios but let me give you
another scenario for example it's absurd
to be honest the fact that we're gonna
have this AI means we're using it for
some purpose and let's hypothetically
say that purpose is to invent things to
[ __ ] solve huge issues that we have
why else would we have the ad well in
one of these solutions or one of these
inventions there could be some
sighs some unintended side effect that
we are not aware of that could be built
into this that could somehow give the AI
power that we're not that we don't want
it to have so for like for example and
it could be something like some [ __ ]
super complex 4d underwater chest [ __ ] I
don't know but essentially we're dealing
with a [ __ ] God you know like he
could predict okay wait a second what if
we make it so that the the only people
that are allowed to interact with the AI
is better AI that we make so the
stronger AI is always keeping the weaker
okay then you're obviously just creating
more problems for the conversation to
distract
